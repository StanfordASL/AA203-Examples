{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Dependencies:\n",
    "- System: python3\n",
    "- Python: jupyter, ipywidgets, numpy, matplotlib, jax\n",
    "\n",
    "Example setup for a Ubuntu system (Mac users, maybe `brew` instead of `sudo apt`; Windows users, learn to love [WSL](https://docs.microsoft.com/en-us/windows/wsl/install-win10)):\n",
    "```\n",
    "/usr/bin/python3 -m pip install --upgrade pip\n",
    "pip install --upgrade jupyter ipywidgets numpy matplotlib jax\n",
    "jupyter nbextension enable --py widgetsnbextension  # necessary for interactive sliders to show up\n",
    "jupyter notebook  # from the directory of this notebook\n",
    "```\n",
    "Alternatively, view this notebook on [Google Colab](https://colab.research.google.com/github/StanfordASL/AA203-Examples/blob/master/Lecture-15/Heuristic-based%20Nonlinear%20MPC%20for%20\"Asteroids\".ipynb) and run a cell containing these commands:\n",
    "```\n",
    "!pip install --upgrade matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np; np.seterr(invalid=\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({'font.size': 20})\n",
    "import matplotlib.collections\n",
    "import matplotlib.transforms\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iLQR implementation from [LQR Variants.ipynb](https://github.com/StanfordASL/AA203-Examples/blob/master/Lecture-10/LQR%20Variants.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `NamedTuple`s are used (more accurately, abused) in this code to minimize dependencies;\n",
    "# much better JAX-compatible choices to fit the archetype of \"parameterized function\"\n",
    "# would be `flax.struct.dataclass` or `equinox.Module`.\n",
    "from typing import Callable, NamedTuple\n",
    "\n",
    "\n",
    "class LinearDynamics(NamedTuple):\n",
    "    f_x: jnp.array  # A\n",
    "    f_u: jnp.array  # B\n",
    "\n",
    "    def __call__(self, x, u, k=None):\n",
    "        f_x, f_u = self\n",
    "        return f_x @ x + f_u @ u if k is None else self[k](x, u)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return jax.tree_map(lambda x: x[key], self)\n",
    "\n",
    "\n",
    "class AffinePolicy(NamedTuple):\n",
    "    l: jnp.array  # l\n",
    "    l_x: jnp.array  # L\n",
    "\n",
    "    def __call__(self, x, k=None):\n",
    "        l, l_x = self\n",
    "        return l + l_x @ x if k is None else self[k](x)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return jax.tree_map(lambda x: x[key], self)\n",
    "\n",
    "\n",
    "class QuadraticCost(NamedTuple):\n",
    "    c: jnp.array  # c\n",
    "    c_x: jnp.array  # q\n",
    "    c_u: jnp.array  # r\n",
    "    c_xx: jnp.array  # Q\n",
    "    c_uu: jnp.array  # R\n",
    "    c_ux: jnp.array  # H.T\n",
    "\n",
    "    @classmethod\n",
    "    def from_pure_quadratic(cls, c_xx, c_uu, c_ux):\n",
    "        return cls(\n",
    "            jnp.zeros((c_xx.shape[:-2])),\n",
    "            jnp.zeros(c_xx.shape[:-1]),\n",
    "            jnp.zeros(c_uu.shape[:-1]),\n",
    "            c_xx,\n",
    "            c_uu,\n",
    "            c_ux,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, u, k=None):\n",
    "        c, c_x, c_u, c_xx, c_uu, c_ux = self\n",
    "        return c + c_x @ x + c_u @ u + x @ c_xx @ x / 2 + u @ c_uu @ u / 2 + u @ c_ux @ x if k is None else self[k](x)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return jax.tree_map(lambda x: x[key], self)\n",
    "\n",
    "\n",
    "class QuadraticStateCost(NamedTuple):\n",
    "    v: jnp.array  # p (scalar)\n",
    "    v_x: jnp.array  # p (vector)\n",
    "    v_xx: jnp.array  # P\n",
    "\n",
    "    @classmethod\n",
    "    def from_pure_quadratic(cls, v_xx):\n",
    "        return cls(\n",
    "            jnp.zeros(v_xx.shape[:-2]),\n",
    "            jnp.zeros(v_xx.shape[:-1]),\n",
    "            v_xx,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, k=None):\n",
    "        v, v_x, v_xx = self\n",
    "        return v + v_x @ x + x @ v_xx @ x / 2 if k is None else self[k](x)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return jax.tree_map(lambda x: x[key], self)\n",
    "\n",
    "\n",
    "def rollout_state_feedback_policy(dynamics, policy, x0, step_range, x_nom=None, u_nom=None):\n",
    "\n",
    "    def scan_fn(x, k):\n",
    "        u = policy(x, k) if x_nom is None else u_nom[k] + policy(x - x_nom[k], k)\n",
    "        x1 = dynamics(x, u, k)\n",
    "        return (x1, (x1, u))\n",
    "\n",
    "    xs, us = jax.lax.scan(scan_fn, x0, step_range)[1]\n",
    "    return jnp.concatenate([x0[None], xs]), us\n",
    "\n",
    "\n",
    "def riccati_step(\n",
    "    current_step_dynamics: LinearDynamics,\n",
    "    current_step_cost: QuadraticCost,\n",
    "    next_state_value: QuadraticStateCost,\n",
    "):\n",
    "    f_x, f_u = current_step_dynamics\n",
    "    c, c_x, c_u, c_xx, c_uu, c_ux = current_step_cost\n",
    "    v, v_x, v_xx = next_state_value\n",
    "\n",
    "    q = c + v\n",
    "    q_x = c_x + f_x.T @ v_x\n",
    "    q_u = c_u + f_u.T @ v_x\n",
    "    q_xx = c_xx + f_x.T @ v_xx @ f_x\n",
    "    q_uu = c_uu + f_u.T @ v_xx @ f_u\n",
    "    q_ux = c_ux + f_u.T @ v_xx @ f_x\n",
    "\n",
    "    l = -jnp.linalg.solve(q_uu, q_u)\n",
    "    l_x = -jnp.linalg.solve(q_uu, q_ux)\n",
    "\n",
    "    current_state_value = QuadraticStateCost(\n",
    "        q - l.T @ q_uu @ l / 2,\n",
    "        q_x - l_x.T @ q_uu @ l,\n",
    "        q_xx - l_x.T @ q_uu @ l_x,\n",
    "    )\n",
    "    current_step_optimal_policy = AffinePolicy(l, l_x)\n",
    "    return current_state_value, current_step_optimal_policy\n",
    "\n",
    "\n",
    "def ensure_positive_definite(a, eps=1e-3):\n",
    "    w, v = jnp.linalg.eigh(a)\n",
    "    return (v * jnp.maximum(w, eps)) @ v.T\n",
    "\n",
    "\n",
    "class TotalCost(NamedTuple):\n",
    "    running_cost: Callable\n",
    "    terminal_cost: Callable\n",
    "\n",
    "    def __call__(self, xs, us):\n",
    "        step_range = jnp.arange(us.shape[0])\n",
    "        return jnp.sum(jax.vmap(self.running_cost)(xs[:-1], us, step_range)) + self.terminal_cost(xs[-1])\n",
    "\n",
    "\n",
    "class EulerIntegrator(NamedTuple):\n",
    "    \"\"\"Discrete time dynamics from time-invariant continuous time dynamics using the Euler method.\"\"\"\n",
    "    ode: Callable\n",
    "    dt: float\n",
    "\n",
    "    @jax.jit\n",
    "    def __call__(self, x, u, k):\n",
    "        return x + self.dt * self.ode(x, u)\n",
    "\n",
    "\n",
    "class RK4Integrator(NamedTuple):\n",
    "    \"\"\"Discrete time dynamics from time-invariant continuous time dynamics using a 4th order Runge-Kutta method.\"\"\"\n",
    "    ode: Callable\n",
    "    dt: float\n",
    "\n",
    "    @jax.jit\n",
    "    def __call__(self, x, u, k):\n",
    "        k1 = self.dt * self.ode(x, u)\n",
    "        k2 = self.dt * self.ode(x + k1 / 2, u)\n",
    "        k3 = self.dt * self.ode(x + k2 / 2, u)\n",
    "        k4 = self.dt * self.ode(x + k3, u)\n",
    "        return x + (k1 + 2 * k2 + 2 * k3 + k4) / 6\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def iterative_linear_quadratic_regulator(dynamics, total_cost, x0, u_guess, maxiter=100, atol=1e-3):\n",
    "    running_cost, terminal_cost = total_cost\n",
    "    n, (N, m) = x0.shape[-1], u_guess.shape\n",
    "    step_range = jnp.arange(N)\n",
    "\n",
    "    xs, us = rollout_state_feedback_policy(dynamics, lambda x, k: u_guess[k], x0, step_range)\n",
    "    j = total_cost(xs, us)\n",
    "\n",
    "    def continuation_criterion(loop_vars):\n",
    "        i, _, _, j_curr, j_prev = loop_vars\n",
    "        return (j_curr < j_prev - atol) & (i < maxiter)\n",
    "\n",
    "    def ilqr_iteration(loop_vars):\n",
    "        i, xs, us, j_curr, j_prev = loop_vars\n",
    "\n",
    "        f_x, f_u = jax.vmap(jax.jacobian(dynamics, (0, 1)))(xs[:-1], us, step_range)\n",
    "        c = jax.vmap(running_cost)(xs[:-1], us, step_range)\n",
    "        c_x, c_u = jax.vmap(jax.grad(running_cost, (0, 1)))(xs[:-1], us, step_range)\n",
    "        (c_xx, c_xu), (c_ux, c_uu) = jax.vmap(jax.hessian(running_cost, (0, 1)))(xs[:-1], us, step_range)\n",
    "        v, v_x, v_xx = terminal_cost(xs[-1]), jax.grad(terminal_cost)(xs[-1]), jax.hessian(terminal_cost)(xs[-1])\n",
    "\n",
    "        # Ensure quadratic cost terms are positive definite.\n",
    "        c_zz = jnp.block([[c_xx, c_xu], [c_ux, c_uu]])\n",
    "        c_zz = jax.vmap(ensure_positive_definite)(c_zz)\n",
    "        c_xx, c_uu, c_ux = c_zz[:, :n, :n], c_zz[:, -m:, -m:], c_zz[:, -m:, :n]\n",
    "        v_xx = ensure_positive_definite(v_xx)\n",
    "\n",
    "        linearized_dynamics = LinearDynamics(f_x, f_u)\n",
    "        quadratized_running_cost = QuadraticCost(c, c_x, c_u, c_xx, c_uu, c_ux)\n",
    "        quadratized_terminal_cost = QuadraticStateCost(v, v_x, v_xx)\n",
    "\n",
    "        def scan_fn(next_state_value, current_step_dynamics_cost):\n",
    "            current_step_dynamics, current_step_cost = current_step_dynamics_cost\n",
    "            current_state_value, current_step_policy = riccati_step(\n",
    "                current_step_dynamics,\n",
    "                current_step_cost,\n",
    "                next_state_value,\n",
    "            )\n",
    "            return current_state_value, current_step_policy\n",
    "\n",
    "        policy = jax.lax.scan(scan_fn,\n",
    "                              quadratized_terminal_cost, (linearized_dynamics, quadratized_running_cost),\n",
    "                              reverse=True)[1]\n",
    "\n",
    "        def rollout_linesearch_policy(alpha):\n",
    "            # Note that we roll out the true `dynamics`, not the `linearized_dynamics`!\n",
    "            l, l_x = policy\n",
    "            return rollout_state_feedback_policy(dynamics, AffinePolicy(alpha * l, l_x), x0, step_range, xs, us)\n",
    "\n",
    "        # Backtracking line search (step sizes evaluated in parallel).\n",
    "        all_xs, all_us = jax.vmap(rollout_linesearch_policy)(0.5**jnp.arange(16))\n",
    "        js = jax.vmap(total_cost)(all_xs, all_us)\n",
    "        a = jnp.argmin(js)\n",
    "        j = js[a]\n",
    "        xs = jnp.where(j < j_curr, all_xs[a], xs)\n",
    "        us = jnp.where(j < j_curr, all_us[a], us)\n",
    "        return i + 1, xs, us, jnp.minimum(j, j_curr), j_curr\n",
    "\n",
    "    i, xs, us, j, _ = jax.lax.while_loop(continuation_criterion, ilqr_iteration, (0, xs, us, j, jnp.inf))\n",
    "\n",
    "    return {\n",
    "        \"optimal_trajectory\": (xs, us),\n",
    "        \"optimal_cost\": j,\n",
    "        \"num_iterations\": i,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Asteroids\" setup\n",
    "It turns out that \"Asteroids\" isn't very fun if you can't shoot the asteroids out of your way as you're coasting.\n",
    "Feel free to play with `ContinuousTimeSpaceshipDynamics` (you may have to drastically reduce the number of asteroids, see the last section below);\n",
    "otherwise this notebook uses the extra maneuverability offered by `ContinuousTimeRocketCarDynamics` to illustrate more\n",
    "interesting MPC behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousTimeSpaceshipDynamics(NamedTuple):\n",
    "\n",
    "    def __call__(self, state, control):\n",
    "        x, y, q, v_x, v_y = state\n",
    "        r, a = control\n",
    "        return jnp.array([\n",
    "            v_x,\n",
    "            v_y,\n",
    "            r,\n",
    "            a * jnp.cos(q),\n",
    "            a * jnp.sin(q),\n",
    "        ])\n",
    "\n",
    "\n",
    "class ContinuousTimeRocketCarDynamics(NamedTuple):\n",
    "\n",
    "    def __call__(self, state, control):\n",
    "        x, y, q, v = state\n",
    "        r, a = control\n",
    "        return jnp.array([\n",
    "            v * jnp.cos(q),\n",
    "            v * jnp.sin(q),\n",
    "            r,  # Note that these dynamics allow turning in place, unlike boring terrestrial cars.\n",
    "            a,\n",
    "        ])\n",
    "\n",
    "\n",
    "class Asteroid(NamedTuple):\n",
    "    center: jnp.array\n",
    "    radius: jnp.array\n",
    "    velocity: jnp.array = 0\n",
    "\n",
    "    def at_time(self, time):\n",
    "        return self._replace(center=self.center + self.velocity * time)\n",
    "\n",
    "\n",
    "class Environment(NamedTuple):\n",
    "    asteroids: Asteroid\n",
    "    ship_radius: jnp.array\n",
    "    sensing_radius: jnp.array\n",
    "    bounds: jnp.array\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, num_asteroids, ship_radius=1.0, sensing_radius=5, bounds=(50, 40)):\n",
    "        bounds = np.array(bounds)\n",
    "        return cls(\n",
    "            Asteroid(\n",
    "                np.random.rand(num_asteroids, 2) * bounds,\n",
    "                1 + 2 * np.random.rand(num_asteroids),\n",
    "                np.random.randn(num_asteroids, 2),\n",
    "            ), ship_radius, sensing_radius, bounds)\n",
    "\n",
    "    def at_time(self, time):\n",
    "        return self._replace(asteroids=self.asteroids.at_time(time))\n",
    "\n",
    "    def wrap_vector(self, vector):\n",
    "        return (vector + self.bounds / 2) % self.bounds - self.bounds / 2\n",
    "\n",
    "    def sense(self, position):\n",
    "        deltas = self.wrap_vector(position - self.asteroids.center)\n",
    "        return self._replace(asteroids=self.asteroids._replace(radius=jnp.where(\n",
    "            jnp.linalg.norm(deltas, axis=-1) -\n",
    "            self.asteroids.radius < self.sensing_radius, self.asteroids.radius, np.nan)))\n",
    "\n",
    "    def plot(self, state=None, plan=None, history=None, sensor=False, scaled_thrust=None, ax=None):\n",
    "        pose = np.full(3, np.nan) if state is None else state[:3]\n",
    "        plan = np.full((0, 2), np.nan) if plan is None else plan[:, :2]\n",
    "        history = np.full((0, 2), np.nan) if history is None else history[:, :2]\n",
    "        scaled_thrust = np.full((), np.nan) if scaled_thrust is None else scaled_thrust\n",
    "\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            ax.set_xlim(0, self.bounds[0])\n",
    "            ax.set_ylim(0, self.bounds[1])\n",
    "            ax.set_aspect(1)\n",
    "            asteroids = ax.add_collection(\n",
    "                matplotlib.collections.PatchCollection(\n",
    "                    [plt.Circle(np.zeros(2), r) for r in self.asteroids.radius] * 4,\n",
    "                    offsets=np.zeros(2),\n",
    "                    transOffset=matplotlib.transforms.AffineDeltaTransform(ax.transData),\n",
    "                    color=\"black\",\n",
    "                ))\n",
    "            ship = ax.add_collection(\n",
    "                matplotlib.collections.PatchCollection(\n",
    "                    [plt.Polygon(self.ship_radius * np.array([[-.2, -.4], [1., 0], [-.2, .4]]))] * 4,\n",
    "                    offsets=np.zeros(2),\n",
    "                    transOffset=matplotlib.transforms.AffineDeltaTransform(ax.transData),\n",
    "                    color=\"orange\",\n",
    "                    zorder=10,\n",
    "                ))\n",
    "            circle = ax.add_collection(\n",
    "                matplotlib.collections.PatchCollection(\n",
    "                    [plt.Circle(np.zeros(2), self.sensing_radius)] * 4,\n",
    "                    offsets=np.zeros(2),\n",
    "                    transOffset=matplotlib.transforms.AffineDeltaTransform(ax.transData),\n",
    "                    facecolor=(0, 0, 0, 0),\n",
    "                    edgecolor=\"black\",\n",
    "                    linestyle=\"--\",\n",
    "                    zorder=10,\n",
    "                ))\n",
    "            thruster = ax.add_collection(\n",
    "                matplotlib.collections.PatchCollection(\n",
    "                    [plt.Polygon(self.ship_radius * np.array([[-1., 0.], [0., -.25], [0., .25]]))] * 4,\n",
    "                    offsets=np.zeros(2),\n",
    "                    transOffset=matplotlib.transforms.AffineDeltaTransform(ax.transData),\n",
    "                    color=\"red\",\n",
    "                    zorder=5,\n",
    "                ))\n",
    "            plan_line = ax.plot(plan[:, 0], plan[:, 1], color=\"green\")[0]\n",
    "            history_line = ax.plot(history[:, 0], history[:, 1], color=\"blue\")[0]\n",
    "        else:\n",
    "            fig = ax.figure\n",
    "            asteroids, ship, circle, thruster = ax.collections\n",
    "            plan_line, history_line = ax.lines\n",
    "        screen_offsets = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "        asteroids.set_offsets(\n",
    "            (self.wrap_vector(self.asteroids.center) + self.bounds * screen_offsets[:, None, :]).reshape(-1, 2))\n",
    "        if sensor:\n",
    "            asteroids.set_alpha(np.where(np.isnan(self.sense(pose[:2]).asteroids.radius), 0.1, 1.0))\n",
    "        ship.set_offsets(self.wrap_vector(pose[:2]) + self.bounds * screen_offsets)\n",
    "        ship.set_transform(matplotlib.transforms.Affine2D().rotate(pose[2]) + ax.transData)\n",
    "        circle.set_offsets(self.wrap_vector(pose[:2] if sensor else np.full(2, np.nan)) + self.bounds * screen_offsets)\n",
    "        thruster.set_offsets(self.wrap_vector(pose[:2]) + self.bounds * screen_offsets)\n",
    "        thruster.set_transform(matplotlib.transforms.Affine2D().scale(0.2 + 0.8 * scaled_thrust, 1).rotate(pose[2]) +\n",
    "                               ax.transData)\n",
    "\n",
    "        def tile_line(line):\n",
    "            if line.shape[0] == 0:\n",
    "                return line\n",
    "            irange, jrange = [\n",
    "                range(int(x[0]), int(x[1] + 1))\n",
    "                for x in zip(np.min(line, 0) // self.bounds,\n",
    "                             np.max(line, 0) // self.bounds)\n",
    "            ]\n",
    "            return np.concatenate([\n",
    "                np.pad(line - np.array([i, j]) * self.bounds, ((0, 1), (0, 0)), constant_values=np.nan)\n",
    "                for i in irange\n",
    "                for j in jrange\n",
    "            ], 0)\n",
    "\n",
    "        plan_line.set_data(*tile_line(plan).T)\n",
    "        history_line.set_data(*tile_line(history).T)\n",
    "        return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem parameters.\n",
    "T = 125\n",
    "dt = 0.1\n",
    "dynamics = RK4Integrator(ContinuousTimeRocketCarDynamics(), dt)\n",
    "start_state = np.array([5., 5., 0., 0.])\n",
    "goal_position = np.array([45., 35.])\n",
    "\n",
    "np.random.seed(2)\n",
    "env = Environment.create(20)\n",
    "fig, ax = env.plot()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "@interact(k=(0, T))\n",
    "def plot(k=0):\n",
    "    env.at_time(k * dt).plot(ax=ax)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Optimization with Perfect Information (Full Horizon `T`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningCost(NamedTuple):\n",
    "    env: Environment\n",
    "    dt: jnp.array\n",
    "\n",
    "    def __call__(self, state, control, step):\n",
    "        # NOTE: many parameters (gains, offsets) in this function could be lifted to fields of `RunningCost`, in which\n",
    "        # case you could experiment with changing these parameters without incurring `jax.jit` recompilation.\n",
    "        asteroids = self.env.asteroids.at_time(step * self.dt)\n",
    "\n",
    "        separation_distance = jnp.where(\n",
    "            jnp.isnan(asteroids.radius), np.inf,\n",
    "            jnp.linalg.norm(self.env.wrap_vector(state[:2] - asteroids.center), axis=-1) - asteroids.radius -\n",
    "            self.env.ship_radius)\n",
    "        collision_avoidance_penalty = jnp.sum(\n",
    "            jnp.where(separation_distance > 0.3, 0, 1e4 * (0.3 - separation_distance)**2))\n",
    "\n",
    "        r, a = control\n",
    "        yaw_rate_penalty = 1e4 * jnp.maximum(jnp.abs(r) - np.pi / 2, 0)**2\n",
    "        acceleration_penalty = 1e4 * jnp.maximum(jnp.abs(a) - 4, 0)**2  # explicitly allowing for deceleration.\n",
    "\n",
    "        return collision_avoidance_penalty + yaw_rate_penalty + acceleration_penalty + r**2 + a**2\n",
    "\n",
    "\n",
    "class FullHorizonTerminalCost(NamedTuple):\n",
    "    env: Environment\n",
    "    goal_position: jnp.array\n",
    "\n",
    "    @classmethod\n",
    "    def create_ignoring_extra_args(cls, env, goal_position, *args, **kwargs):\n",
    "        return cls(env, goal_position)\n",
    "\n",
    "    def __call__(self, state):\n",
    "        return 1000 * (jnp.sum(jnp.square(state[:2] - self.goal_position)) + state[3]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we incrementally add constraints to help trajectory optimization converge to a decent solution; i.e., we solve\n",
    "# first in a completely empty environment (no asteroids) and use this trajectory to warm start the optimization in the\n",
    "# populated environment.\n",
    "# As we'll see in later cells, this isn't the most robust way to initialize iLQR; you can still get stuck in local\n",
    "# minima (e.g., where you just plow through the thinnest point of a cluster of asteroids). Enterprising students may\n",
    "# consider experimenting with multiple random initializations (if you use `jax.vmap`, this is more-or-less trivial to\n",
    "# code!) or otherwise more complicated search methods (e.g., graph-based) for initializating the control sequence.\n",
    "empty_env = Environment.create(0)\n",
    "solution = iterative_linear_quadratic_regulator(\n",
    "    dynamics,\n",
    "    TotalCost(RunningCost(empty_env, dt), FullHorizonTerminalCost(empty_env, goal_position)),\n",
    "    start_state,\n",
    "    np.zeros((T, 2)),\n",
    ")\n",
    "solution = iterative_linear_quadratic_regulator(\n",
    "    dynamics,\n",
    "    TotalCost(RunningCost(env, dt), FullHorizonTerminalCost(env, goal_position)),\n",
    "    start_state,\n",
    "    solution[\"optimal_trajectory\"][1],\n",
    ")\n",
    "solution = jax.tree_map(np.array, solution)\n",
    "states, controls = solution[\"optimal_trajectory\"]\n",
    "fig, ax = env.plot()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "@interact(k=(0, T))\n",
    "def plot(k=0):\n",
    "    env.at_time(k * dt).plot(states[k], states[:k + 1], states[k:], ax=ax)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(T) * dynamics.dt, controls, label=[\"yaw rate\", \"acceleration\"])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPC with Perfect Information (Horizon `N`)\n",
    "In case optimizing over the full horizon is too expensive (or perhaps there is no notion of \"full horizon\"), we consider now receding horizon control. Other reasons to consider an MPC formulation could include, e.g.,\n",
    "- disturbance rejection (though there is no noise added in the simulations below),\n",
    "- environmental uncertainty (see the next section on \"MPC with Partial Observability\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=[\"running_cost_type\", \"terminal_cost_type\", \"limited_sensing\", \"N\"])\n",
    "def policy(state, env, dynamics, running_cost_type, terminal_cost_type, limited_sensing=False, N=20):\n",
    "    if limited_sensing:\n",
    "        env = env.sense(state[:2])\n",
    "    empty_env = Environment.create(0)\n",
    "    solution = iterative_linear_quadratic_regulator(\n",
    "        dynamics,\n",
    "        TotalCost(\n",
    "            running_cost_type(empty_env, dynamics.dt),\n",
    "            terminal_cost_type.create_ignoring_extra_args(\n",
    "                empty_env,\n",
    "                goal_position,\n",
    "                state[:2],\n",
    "                empty_env.sensing_radius,\n",
    "            ),\n",
    "        ),\n",
    "        state,\n",
    "        np.zeros((N, 2)),\n",
    "    )\n",
    "    solution = iterative_linear_quadratic_regulator(\n",
    "        dynamics,\n",
    "        TotalCost(\n",
    "            running_cost_type(env, dynamics.dt),\n",
    "            terminal_cost_type.create_ignoring_extra_args(\n",
    "                env,\n",
    "                goal_position,\n",
    "                state[:2],\n",
    "                env.sensing_radius,\n",
    "            ),\n",
    "        ),\n",
    "        state,\n",
    "        solution[\"optimal_trajectory\"][1],\n",
    "    )\n",
    "    states, controls = solution[\"optimal_trajectory\"]\n",
    "    return controls[0], (states, controls)\n",
    "\n",
    "\n",
    "def simulate_mpc(start_state, env, dynamics, running_cost_type, terminal_cost_type, limited_sensing=False, N=20, T=T):\n",
    "    states = [start_state]\n",
    "    controls = []\n",
    "    plans = []\n",
    "    for t in range(T):\n",
    "        control, (mpc_states, mpc_controls) = policy(states[-1], env.at_time(t * dynamics.dt), dynamics,\n",
    "                                                     running_cost_type, terminal_cost_type, limited_sensing, N)\n",
    "        states.append(mpc_states[1])\n",
    "        controls.append(control)\n",
    "        plans.append(mpc_states)\n",
    "    states = np.array(states)\n",
    "    controls = np.array(controls)\n",
    "    plans = np.array(plans)\n",
    "\n",
    "    plt.plot(np.arange(T) * dynamics.dt, controls, label=[\"yaw rate\", \"acceleration\"])\n",
    "    plt.legend()\n",
    "    fig, ax = env.plot()\n",
    "    plt.close()\n",
    "\n",
    "    @interact(k=(0, T))\n",
    "    def plot(k=0):\n",
    "        env.at_time(k * dt).plot(\n",
    "            states[k],\n",
    "            states[:k + 1],\n",
    "            plans[k] if k < T else None,\n",
    "            sensor=limited_sensing,\n",
    "            ax=ax,\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `FullHorizonTerminalCost` (overagressive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_mpc(start_state, env, dynamics, RunningCost, FullHorizonTerminalCost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a heuristic `MPCTerminalCost` (better, but requires tuning)\n",
    "This is a completely made-up, hand-constructed terminal cost function for illustrative purposes.\n",
    "More principled approaches might include:\n",
    "- working out some math to compute the cost-to-go (value function) for a simple tail policy (e.g., turn towards the goal and drive straight there),\n",
    "- learning a terminal cost function from experience,\n",
    "- more generally, using a direct method that can properly incorporate constraints, e.g., an augmented Lagrangian formulation/outer loop around iLQR, to disambiguate constraints (all currently soft) from costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPCTerminalCost(NamedTuple):\n",
    "    env: Environment\n",
    "    goal_position: jnp.array\n",
    "\n",
    "    @classmethod\n",
    "    def create_ignoring_extra_args(cls, env, goal_position, *args, **kwargs):\n",
    "        return cls(env, goal_position)\n",
    "\n",
    "    def __call__(self, state):\n",
    "        # We use a \"Huber loss\"-like penalty to encourage goal-seeking behavior; an L1 norm penalty on the terminal\n",
    "        # state makes it so that early MPC iterations that are further from the goal don't feel a stronger \"pull\" than\n",
    "        # later iterations.\n",
    "        distance_to_goal = jnp.linalg.norm(state[:2] - self.goal_position)\n",
    "        goal_penalty = jnp.where(distance_to_goal > 1, 2 * distance_to_goal - 1, distance_to_goal**2)\n",
    "        speed_penalty = state[3]**2\n",
    "        return 50 * (goal_penalty + speed_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_mpc(start_state, env, dynamics, RunningCost, MPCTerminalCost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPC with Partial Observability (Horizon `N`)\n",
    "Now we consider a limited sensing radius on our ship -- this is a stronger reason to apply MPC than, e.g., simply disturbance rejection (though I should note that MPC for tracking with constraints, which is a much more straightforward application of our discussions on linear MPC, is a topic for another notebook, another day...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the same `MPCTerminalCost` with limited sensing (now too bold!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_mpc(start_state, env, dynamics, RunningCost, MPCTerminalCost, limited_sensing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More heuristic tuning to the rescue with `MPCTerminalCostAndSoftConstraint`\n",
    "A more principled approach here could be to use reachability theory to reason about safe states from which you know you can avoid collision with any unforeseen asteroid (at least in a one-by-one fashion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPCTerminalCostAndSoftConstraint(NamedTuple):\n",
    "    env: Environment\n",
    "    goal_position: jnp.array\n",
    "    start_position: jnp.array\n",
    "    trust_region_radius: jnp.array\n",
    "\n",
    "    @classmethod\n",
    "    def create_ignoring_extra_args(cls, *args, **kwargs):\n",
    "        return cls(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, state):\n",
    "        # We use a \"Huber loss\"-like penalty to encourage goal-seeking behavior; an L1 norm penalty on the terminal\n",
    "        # state makes it so that early MPC iterations that are further from the goal don't feel a stronger \"pull\" than\n",
    "        # later iterations.\n",
    "        distance_to_goal = jnp.linalg.norm(state[:2] - self.goal_position)\n",
    "        goal_penalty = jnp.where(distance_to_goal > 1, 2 * distance_to_goal - 1, distance_to_goal**2)\n",
    "        speed_penalty = state[3]**2\n",
    "\n",
    "        # With limited sensing, we also now enforce a trust region to stay near where we start instead of having MPC\n",
    "        # plans charge off into the great unknown.\n",
    "        distance_from_start = jnp.sqrt(jnp.sum(jnp.square(state[:2] - self.start_position)) + 1e-3)\n",
    "        trust_region_penalty = jnp.where(distance_from_start < self.trust_region_radius, 0,\n",
    "                                         (distance_from_start - self.trust_region_radius)**2)\n",
    "\n",
    "        return 50 * (goal_penalty + speed_penalty) + 1000 * trust_region_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_mpc(start_state, env, dynamics, RunningCost, MPCTerminalCostAndSoftConstraint, limited_sensing=True, T=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with `ContinuousTimeSpaceshipDynamics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem parameters.\n",
    "T = 100\n",
    "dt = 0.1\n",
    "start_state = np.array([5., 5., 0., 0., 0.])\n",
    "goal_position = np.array([45., 35.])\n",
    "\n",
    "np.random.seed(7)\n",
    "env = Environment.create(10, ship_radius=2.0)\n",
    "fig, ax = env.plot()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "@interact(k=(0, T))\n",
    "def plot(k=0):\n",
    "    env.at_time(k * dt).plot(ax=ax)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningCost(NamedTuple):\n",
    "    env: Environment\n",
    "    dt: jnp.array\n",
    "\n",
    "    def __call__(self, state, control, step):\n",
    "        asteroids = self.env.asteroids.at_time(step * self.dt)\n",
    "\n",
    "        position = state[:2]\n",
    "        separation_distance = jnp.where(\n",
    "            jnp.isnan(asteroids.radius), np.inf,\n",
    "            jnp.linalg.norm(self.env.wrap_vector(position - asteroids.center), axis=-1) - asteroids.radius -\n",
    "            self.env.ship_radius)\n",
    "        collision_avoidance_penalty = jnp.sum(\n",
    "            jnp.where(separation_distance > 0.2, 0, 1e4 * (0.2 - separation_distance)**2))\n",
    "\n",
    "        r, a = control\n",
    "        yaw_rate_limit = 1e4 * jnp.maximum(jnp.abs(r) - np.pi, 0)**2\n",
    "        acceleration_limit = 1e4 * jnp.maximum(jnp.abs(a - 2) - 2, 0)**2  # all gas; no brakes.\n",
    "\n",
    "        return collision_avoidance_penalty + yaw_rate_limit + acceleration_limit + r**2 + a**2\n",
    "\n",
    "\n",
    "class FullHorizonTerminalCost(NamedTuple):\n",
    "    env: Environment\n",
    "    goal_position: jnp.array\n",
    "\n",
    "    def __call__(self, state):\n",
    "        return 1000 * (jnp.sum(jnp.square(state[:2] - self.goal_position)) + state[3]**2 + state[4]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_env = Environment.create(0)\n",
    "solution = iterative_linear_quadratic_regulator(\n",
    "    RK4Integrator(ContinuousTimeSpaceshipDynamics(), dt),\n",
    "    TotalCost(RunningCost(empty_env, dt), FullHorizonTerminalCost(empty_env, goal_position)),\n",
    "    start_state,\n",
    "    np.zeros((T, 2)),\n",
    ")\n",
    "solution = iterative_linear_quadratic_regulator(\n",
    "    RK4Integrator(ContinuousTimeSpaceshipDynamics(), dt),\n",
    "    TotalCost(RunningCost(env, dt), FullHorizonTerminalCost(env, goal_position)),\n",
    "    start_state,\n",
    "    solution[\"optimal_trajectory\"][1],\n",
    ")\n",
    "solution = jax.tree_map(np.array, solution)\n",
    "states, controls = solution[\"optimal_trajectory\"]\n",
    "fig, ax = env.plot()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "@interact(k=(0, T))\n",
    "def plot(k=0):\n",
    "    env.at_time(k * dt).plot(\n",
    "        states[k],\n",
    "        states[:k + 1],\n",
    "        states[k:],\n",
    "        sensor=False,\n",
    "        scaled_thrust=controls[k][1] / 4 if k < T else None,\n",
    "        ax=ax,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(T) * dynamics.dt, controls, label=[\"yaw rate\", \"acceleration\"])\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
