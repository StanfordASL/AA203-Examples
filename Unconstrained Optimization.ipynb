{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Dependencies:\n",
    "- System: python3\n",
    "- Python: jupyter, numpy, matplotlib, jax (for autodifferentiation)\n",
    "\n",
    "Example setup for a Ubuntu system (Mac users, maybe `brew` instead of `sudo apt`; Windows users, learn to love [WSL](https://docs.microsoft.com/en-us/windows/wsl/install-win10)):\n",
    "```\n",
    "/usr/bin/python3 -m pip install --upgrade pip\n",
    "pip install --upgrade jupyter numpy matplotlib jax jaxlib\n",
    "jupyter notebook  # from the directory of this notebook\n",
    "```\n",
    "Alternatively, view this notebook on [Google Colab](https://colab.research.google.com/github/StanfordASL/AA203-Examples/blob/master/Lecture-3/Unconstrained%20Optimization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.jet()\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting code.\n",
    "\n",
    "\n",
    "def backtracking_line_search(f, x0, p, k_max=10):\n",
    "    # Normally this would be implemented with a `while` loop/appropriate exit conditions.\n",
    "    xs = x0 + 2.0**(-np.arange(k_max))[:, np.newaxis] * p\n",
    "    return xs[jnp.argmin(jax.vmap(f)(xs))]\n",
    "\n",
    "\n",
    "def gradient_descent(f, descent_direction, x0, num_steps, constant_stepsize=None):\n",
    "    xs = [x0]\n",
    "    # Would be better implemented with `jax.lax.scan`.\n",
    "    for _ in range(num_steps):\n",
    "        x = xs[-1]\n",
    "        p = descent_direction(x)\n",
    "        if constant_stepsize is not None:\n",
    "            xs.append(x + p * constant_stepsize)\n",
    "        else:\n",
    "            xs.append(backtracking_line_search(f, x, p))\n",
    "    return jnp.array(xs)\n",
    "\n",
    "\n",
    "class InteractiveContourPlot:\n",
    "\n",
    "    def __init__(self, f, x_range, y_range, *args, **kwargs):\n",
    "        self.f = f\n",
    "        self.X, self.Y = np.meshgrid(np.linspace(*x_range, 100), np.linspace(*y_range, 100), indexing='ij')\n",
    "\n",
    "        self.fig = plt.figure(*args, **kwargs)\n",
    "        self.ax = self.fig.add_subplot(111)\n",
    "        self.ax.contour(self.X, self.Y, jax.vmap(jax.vmap(f))(np.stack([self.X, self.Y], -1)), levels=50)\n",
    "\n",
    "        self.active = False\n",
    "        self.last_event = None\n",
    "        self.cidpress = self.fig.canvas.mpl_connect('button_press_event', self.on_press)\n",
    "        self.cidpress = self.fig.canvas.mpl_connect('motion_notify_event', self.on_move)\n",
    "\n",
    "    def redraw(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def on_press(self, event):\n",
    "        self.event = event\n",
    "        self.active = not self.active\n",
    "        self.redraw()\n",
    "\n",
    "    def on_move(self, event):\n",
    "        self.event = event\n",
    "        self.redraw()\n",
    "\n",
    "\n",
    "class DescentDirectionsPlot(InteractiveContourPlot):\n",
    "\n",
    "    def __init__(self, f, x_range, y_range, *args, **kwargs):\n",
    "        super().__init__(f, x_range, y_range, *args, **kwargs)\n",
    "        self.neg_grad_f = jax.jit(lambda x: -jax.grad(f)(x))\n",
    "        self.newton_f = jax.jit(lambda x: -jnp.linalg.solve(jax.hessian(f)(x), jax.grad(f)(x)))\n",
    "        self.descent_directions_plot = None\n",
    "\n",
    "    def redraw(self):\n",
    "        if not self.active:\n",
    "            return\n",
    "        x = np.array([self.event.xdata, self.event.ydata])\n",
    "        neg_grad_f_x = self.neg_grad_f(x)\n",
    "        newton_f_x = self.newton_f(x)\n",
    "        if self.descent_directions_plot:\n",
    "            self.descent_directions_plot.set_offsets([x, x])\n",
    "            self.descent_directions_plot.set_UVC([neg_grad_f_x[0], newton_f_x[0]], [neg_grad_f_x[1], newton_f_x[1]])\n",
    "        else:\n",
    "            self.descent_directions_plot = self.ax.quiver([x[0], x[0]], [x[1], x[1]], [neg_grad_f_x[0], newton_f_x[0]],\n",
    "                                                          [neg_grad_f_x[1], newton_f_x[1]],\n",
    "                                                          angles='xy',\n",
    "                                                          scale_units='xy',\n",
    "                                                          scale=1,\n",
    "                                                          width=0.005,\n",
    "                                                          color=['black', 'red'])\n",
    "\n",
    "\n",
    "class GradientDescentPlot(InteractiveContourPlot):\n",
    "\n",
    "    def __init__(self,\n",
    "                 f,\n",
    "                 x_range,\n",
    "                 y_range,\n",
    "                 descent_direction='grad',\n",
    "                 num_steps=10,\n",
    "                 constant_stepsize=None,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        super().__init__(f, x_range, y_range, *args, **kwargs)\n",
    "        self.color = 'black'\n",
    "        if descent_direction == 'grad':\n",
    "\n",
    "            def descent_direction_fn(x):\n",
    "                grad_f_x = jax.grad(f)(x)\n",
    "                # Normalize the gradient; in general this is optional (and maybe irrelevant depending\n",
    "                # on line search details).\n",
    "                return -grad_f_x / jnp.linalg.norm(grad_f_x)\n",
    "\n",
    "        elif descent_direction == 'newton':\n",
    "            self.color = 'red'\n",
    "\n",
    "            def descent_direction_fn(x):\n",
    "                hess_f_x = jax.hessian(f)(x)\n",
    "                # Ensure the hessian is positive definite by adding an appropriate multiple of the\n",
    "                # identity matrix (if necessary). This ensures the returned direction is indeed a descent\n",
    "                # direction (i.e., `np.dot(direction, grad_f_x) < 0`).\n",
    "                min_hess_f_x_eigenvalue = jnp.linalg.eigvalsh(hess_f_x)[0]\n",
    "                pos_def_hess_f_x = hess_f_x + jnp.where(min_hess_f_x_eigenvalue < 1e-3,\n",
    "                                                        (-min_hess_f_x_eigenvalue + 1e-3) * jnp.eye(2), 0.)\n",
    "                return -jnp.linalg.solve(pos_def_hess_f_x, jax.grad(f)(x))\n",
    "\n",
    "        self.gradient_descent = jax.jit(\n",
    "            lambda x0: gradient_descent(f, descent_direction_fn, x0, num_steps, constant_stepsize))\n",
    "        self.descent_trace_plot = None\n",
    "\n",
    "    def redraw(self):\n",
    "        if not self.active:\n",
    "            return\n",
    "        x0 = np.array([self.event.xdata, self.event.ydata])\n",
    "        xs = self.gradient_descent(x0)\n",
    "        diffs = np.diff(xs, axis=0)\n",
    "        if self.descent_trace_plot:\n",
    "            self.descent_trace_plot.set_offsets(xs[:-1])\n",
    "            self.descent_trace_plot.set_UVC(diffs[:, 0], diffs[:, 1])\n",
    "        else:\n",
    "            self.descent_trace_plot = self.ax.quiver(xs[:-1, 0],\n",
    "                                                     xs[:-1, 1],\n",
    "                                                     diffs[:, 0],\n",
    "                                                     diffs[:, 1],\n",
    "                                                     angles='xy',\n",
    "                                                     scale_units='xy',\n",
    "                                                     scale=1,\n",
    "                                                     width=0.005,\n",
    "                                                     color=self.color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some interesting functions.\n",
    "\n",
    "\n",
    "def gaussian_mixture_model_logpdf(x, weights, means, covariances):\n",
    "    log_probs = jax.scipy.stats.multivariate_normal.logpdf(x, means, covariances)\n",
    "    return jax.scipy.special.logsumexp(log_probs, b=weights)\n",
    "\n",
    "\n",
    "def two_local_minima(x):\n",
    "    # Makes for an interesting-looking optimization landscape; details unimportant.\n",
    "    np.random.seed(0)\n",
    "    cov_factor = np.eye(2)[np.newaxis] * np.array([.3, .4, .6])[:, np.newaxis,\n",
    "                                                                np.newaxis] + (np.random.rand(3, 2, 2) - 0.5) / 2\n",
    "\n",
    "    return -gaussian_mixture_model_logpdf(x, np.array([0.1, 0.2, 0.7]), np.array([[-.7, .7], [0., .6], [.8, -.8]]),\n",
    "                                          np.matmul(cov_factor, cov_factor.swapaxes(-1, -2))) / 10\n",
    "\n",
    "\n",
    "def rosenbrock(x):\n",
    "    return ((1 - x[0])**2 + 4 * (x[1] - x[0]**2)**2) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click to activate/deactivate descent direction visualization (you may have to wait a bit for JAX compilation).\n",
    "# Black: gradient\n",
    "# Red: direction from Newton's method (only guaranteed to be a descent direction if the Hessian is positive\n",
    "# definite; visualized in all cases anyway)\n",
    "DescentDirectionsPlot(\n",
    "    two_local_minima,  # try also `rosenbrock`\n",
    "    (-1.5, 1.5),\n",
    "    (-1.5, 1.5),\n",
    "    figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click to activate/deactivate gradient descent visualization (you may have to wait a bit for JAX compilation).\n",
    "GradientDescentPlot(\n",
    "    two_local_minima,  # try also `rosenbrock`\n",
    "    (-1.5, 1.5),\n",
    "    (-1.5, 1.5),\n",
    "    'grad',  # try 'grad' for steepest descent, 'newton' for Newton's method\n",
    "    num_steps=15,\n",
    "    constant_stepsize=None,  # `None` for backtracking line search, otherwise try, e.g., `0.1`\n",
    "    figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
